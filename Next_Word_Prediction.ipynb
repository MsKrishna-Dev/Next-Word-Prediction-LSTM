{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYrclXaHxDvD",
        "outputId": "710d9459-2acb-4b5e-bc99-f6787a24ae4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O9YIeDxN1GeJUt62aK-weybfTVW5Hwj_\n",
            "To: /content/1661-0.txt\n",
            "100% 608k/608k [00:00<00:00, 107MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown --id 1O9YIeDxN1GeJUt62aK-weybfTVW5Hwj_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oVs8_Od13si",
        "outputId": "c9932168-72ac-423e-da1d-3e5efd140f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text preview:\n",
            " start of this project gutenberg ebook the adventures of sherlock holmes produced by an anonymous project gutenberg volunteer and jose menendez cover the adventures of sherlock holmes by arthur conan doyle contents i. a scandal in bohemia ii. the red headed league iii. a case of identity iv. the boscombe valley mystery v. the five orange pips vi. the man with the twisted lip vii. the adventure of the blue carbuncle viii. the adventure of the speckled band ix. the adventure of the engineer s thumb\n",
            "\n",
            "Total length of cleaned text: 552984\n"
          ]
        }
      ],
      "source": [
        "# ------------------------\n",
        "# STEP 1: IMPORT LIBRARIES\n",
        "# ------------------------\n",
        "\n",
        "import re     # 're' is Python's regular expressions library for text cleaning\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# STEP 2: LOAD THE TEXT FILE\n",
        "# ------------------------\n",
        "\n",
        "# The file you downloaded via gdown gets saved with its original name.\n",
        "# file_path = \"1661-0.txt\"\n",
        "\n",
        "# Open and read the entire book into a single string variable\n",
        "with open(\"/content/1661-0.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# STEP 3: EXTRACT ONLY BOOK CONTENT\n",
        "# (remove Project Gutenberg headers/footers)\n",
        "# ------------------------\n",
        "\n",
        "start_marker = \"*** START OF THIS PROJECT\"\n",
        "end_marker   = \"*** END OF THIS PROJECT\"\n",
        "\n",
        "start_idx = text.find(start_marker)\n",
        "end_idx   = text.find(end_marker)\n",
        "\n",
        "# Slice text from START marker to END marker\n",
        "book_text = text[start_idx:end_idx]\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# STEP 4: BASIC CLEANING\n",
        "# (lowercase, remove unusual symbols, collapse whitespace)\n",
        "# ------------------------\n",
        "\n",
        "# Convert to lowercase for uniformity\n",
        "book_text = book_text.lower()\n",
        "\n",
        "# Keep only letters, numbers, spaces, punctuations . , ! ?\n",
        "book_text = re.sub(r\"[^a-z0-9\\s,.!?]\", \" \", book_text)\n",
        "\n",
        "# Replace multiple spaces/newlines with a single space\n",
        "book_text = re.sub(r\"\\s+\", \" \", book_text).strip()\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# STEP 5: DISPLAY SAMPLE OUTPUT\n",
        "# ------------------------\n",
        "\n",
        "print(\"Cleaned text preview:\\n\", book_text[:500])\n",
        "print(\"\\nTotal length of cleaned text:\", len(book_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgmY-qh3OLZZ",
        "outputId": "319bc7b8-3388-4cff-a0e4-ea9ab9445439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words (vocab size): 12003\n",
            "\n",
            "Sample word index mapping (first 20):\n",
            "[('<OOV>', 1), ('the', 2), ('i', 3), ('and', 4), ('to', 5), ('of', 6), ('a', 7), ('in', 8), ('that', 9), ('it', 10), ('was', 11), ('he', 12), ('you', 13), ('his', 14), ('is', 15), ('my', 16), ('have', 17), ('as', 18), ('with', 19), ('had', 20)]\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------\n",
        "# STEP 2: TOKENIZATION AND VOCABULARY SETUP\n",
        "# ------------------------------------------\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "# - filters='' ensures we DO NOT remove punctuation like . , ? !\n",
        "tokenizer = Tokenizer(filters='', lower=True, oov_token=\"<OOV>\")\n",
        "\n",
        "# Fit tokenizer on the cleaned text\n",
        "tokenizer.fit_on_texts([book_text])\n",
        "\n",
        "# Total vocabulary size\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Total unique words (vocab size):\", total_words)\n",
        "\n",
        "# Show first 20 word mappings\n",
        "print(\"\\nSample word index mapping (first 20):\")\n",
        "sample_items = list(tokenizer.word_index.items())[:20]\n",
        "print(sample_items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAsXuoZVomkA",
        "outputId": "eab5c744-be77-4b71-dc58-c5d3faa747c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in the cleaned book: 106055\n",
            "\n",
            "First 50 tokens:\n",
            " [837, 6, 29, 2738, 2739, 3695, 2, 1533, 6, 112, 65, 3696, 43, 45, 5622, 2738, 2739, 5623, 4, 5624, 5625, 1534, 2, 1533, 6, 112, 65, 43, 611, 3697, 3698, 2740, 371, 7, 1095, 8, 1805, 2741, 2, 253, 484, 984, 2742, 7, 146, 6, 2743, 3699, 2, 612]\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------\n",
        "# STEP 3: CONVERT TEXT TO INTEGER SEQUENCES\n",
        "# ------------------------------------------\n",
        "\n",
        "# Convert entire book text into a sequence of integer tokens\n",
        "sequences = tokenizer.texts_to_sequences([book_text])\n",
        "\n",
        "# sequences is a list containing ONE long list → extract it\n",
        "sequences = sequences[0]\n",
        "\n",
        "print(\"Total tokens in the cleaned book:\", len(sequences))\n",
        "\n",
        "# Show first 50 tokens\n",
        "print(\"\\nFirst 50 tokens:\\n\", sequences[:50])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "S-ghTexnpLR0",
        "outputId": "4084400c-066f-4844-9cd8-f02309332c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens = 106055\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'current_sequence' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3298824933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Add this sequence to our list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0minput_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total sequences created =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'current_sequence' is not defined"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------\n",
        "# STEP 4: CREATE INPUT–OUTPUT SEQUENCES (Beginner Friendly)\n",
        "# ------------------------------------------\n",
        "\n",
        "# 1. Convert the book text into numbers (tokens)\n",
        "token_list = tokenizer.texts_to_sequences([book_text])[0]\n",
        "\n",
        "print(\"Total number of tokens =\", len(token_list))\n",
        "\n",
        "\n",
        "# 2. Create MANY sequences where:\n",
        "#    Input = some words\n",
        "#    Output = next word\n",
        "#    Example: [the, dog] → barked\n",
        "\n",
        "input_sequences = []\n",
        "\n",
        "# Start from index 3 (we need at least 2 words before predicting 3rd word)\n",
        "for i in range(3, len(token_list)):\n",
        "\n",
        "    # Create a sequence from the start up to i\n",
        "    # Example: token_list[:5] means first 5 words\n",
        "    # current_sequence = token_list[0:i]\n",
        "\n",
        "    # Add this sequence to our list\n",
        "    input_sequences.append(current_sequence)\n",
        "\n",
        "print(\"Total sequences created =\", len(input_sequences))\n",
        "\n",
        "\n",
        "# 3. Find the maximum length of any sequence\n",
        "# (This is needed so we can pad all sequences to same size)\n",
        "\n",
        "max_seq_len = 0\n",
        "\n",
        "for seq in input_sequences:\n",
        "    if len(seq) > max_seq_len:\n",
        "        max_seq_len = len(seq)\n",
        "\n",
        "print(\"Maximum sequence length =\", max_seq_len)\n",
        "\n",
        "\n",
        "# 4. Pad all sequences on the left side with 0s\n",
        "#    Example: [3, 5, 9] → [0, 0, 3, 5, 9] (to match max length)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_sequences = pad_sequences(\n",
        "    input_sequences,\n",
        "    maxlen=max_seq_len,\n",
        "    padding='pre'   # add 0's on the left\n",
        ")\n",
        "\n",
        "print(\"Padded sequences shape =\", padded_sequences.shape)\n",
        "\n",
        "\n",
        "# 5. Split sequences into Input (X) and Output (y)\n",
        "#    Example sequence: [0, 0, 3, 5, 9]\n",
        "#    X = [0, 0, 3, 5]\n",
        "#    y = 9\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = padded_sequences[:, :-1]   # all words except last\n",
        "y = padded_sequences[:, -1]    # last word only\n",
        "\n",
        "print(\"Shape of X =\", X.shape)\n",
        "print(\"Shape of y =\", y.shape)\n",
        "\n",
        "\n",
        "# 6. Convert y (labels) into one-hot encoding\n",
        "#    Example: word 9 → [0 0 0 0 0 0 0 0 1 0 0 ... ]\n",
        "\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# print(\"Final shape of y =\", y.shape)\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, epochs=50, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmHPyB2UzNKS",
        "outputId": "66b1563a-6343-40ef-bd29-b5b8653191db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens = 106055\n",
            "Total sequences = 106035\n",
            "X shape = (106035, 19)\n",
            "y shape = (106035,)\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m759/829\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m17s\u001b[0m 245ms/step - accuracy: 0.0542 - loss: 7.1689"
          ]
        }
      ],
      "source": [
        "# STEP 4: CREATE INPUT–OUTPUT SEQUENCES (Sliding Window Version)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "\n",
        "\n",
        "token_list = tokenizer.texts_to_sequences([book_text])[0]\n",
        "print(\"Total tokens =\", len(token_list))\n",
        "\n",
        "input_sequences = []\n",
        "window_size = 20   # keep sequences short\n",
        "\n",
        "for i in range(window_size, len(token_list)):\n",
        "    current_sequence = token_list[i-window_size:i]\n",
        "    input_sequences.append(current_sequence)\n",
        "\n",
        "print(\"Total sequences =\", len(input_sequences))\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=window_size, padding='pre')\n",
        "\n",
        "X = padded_sequences[:, :-1]\n",
        "y = padded_sequences[:, -1]\n",
        "\n",
        "print(\"X shape =\", X.shape)\n",
        "print(\"y shape =\", y.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=window_size-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, epochs=50, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ok0xdCUdYcTh"
      },
      "outputs": [],
      "source": [
        "# STEP 5 — TEXT GENERATION\n",
        "\n",
        "def generate_text(seed_text, next_words=30):\n",
        "    for _ in range(next_words):\n",
        "\n",
        "        # Convert seed text → numbers (tokens)\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "        # Pad tokens to match training input size\n",
        "        token_list = pad_sequences([token_list], maxlen=X.shape[1], padding='pre')\n",
        "\n",
        "        # Predict probabilities for next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "\n",
        "        # Pick the word with highest probability\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Convert index → word\n",
        "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
        "\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yDiO9CKGovw1"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: Get Top-5 Most Likely Next Words\n",
        "\n",
        "def predict_top_words(seed_text, top_k=5):\n",
        "    # Convert seed text to tokens\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "    # Pad sequence\n",
        "    token_list = pad_sequences([token_list], maxlen=X.shape[1], padding='pre')\n",
        "\n",
        "    # Predict probabilities for next word\n",
        "    predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "    # Get top-k highest probability indices\n",
        "    top_indices = predicted_probs.argsort()[-top_k:][::-1]\n",
        "\n",
        "    # Convert indices to words\n",
        "    predicted_words = [tokenizer.index_word.get(idx, \"\") for idx in top_indices]\n",
        "\n",
        "    return predicted_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gEXZOImo7FL",
        "outputId": "dd4f8f3b-bbbb-497f-9289-df4c1e829e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter the beginning of a sentence: \n",
            "Please type something…\n",
            "\n",
            "Enter the beginning of a sentence: The Sherlock Holmes entered the\n",
            "\n",
            "Top predictions: ['kitchen', 'red', 'key', 'young', 'bed', 'door', 'house,', 'room.', 'door,', 'left']\n",
            "\n",
            "Do you want to try again? (yes/no): n\n",
            "Exiting… Thank you!\n",
            "Model stopped.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter the beginning of a sentence: \")\n",
        "\n",
        "    # If user enters empty input → skip\n",
        "    if user_input.strip() == \"\":\n",
        "        print(\"Please type something…\")\n",
        "        continue\n",
        "\n",
        "    # Predict next words\n",
        "    predictions = predict_top_words(user_input, top_k=7)\n",
        "    print(\"\\nTop predictions:\", predictions)\n",
        "\n",
        "    # Ask user if they want to continue\n",
        "    choice = input(\"\\nDo you want to try again? (yes/no): \").lower()\n",
        "\n",
        "    if choice in [\"no\", \"n\", \"exit\", \"quit\"]:\n",
        "        print(\"Exiting… Thank you!\")\n",
        "        break\n",
        "\n",
        "print(\"Model stopped.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7xzia9jqpXoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5012f88d-7731-4157-8aef-4780a86c3012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save(\"nwp_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n"
      ],
      "metadata": {
        "id": "0ZnEqMX6Aff3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"nwp_model.h5\")\n",
        "files.download(\"tokenizer.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4ilDieIBBE8o",
        "outputId": "0eba9422-f118-440c-db72-4ed0d3e6cac5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3021b492-0748-4a07-9165-c392ee6d40f3\", \"nwp_model.h5\", 61414076)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_761207e3-8132-448f-8236-d5cf69e9b5fa\", \"tokenizer.pkl\", 478173)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model(\"nwp_model.keras\")\n",
        "\n",
        "# Load tokenizer\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "max_len = 18   # your window_size\n",
        "\n",
        "def predict_words(sentence):\n",
        "    seq = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    seq = seq[-max_len:]\n",
        "    seq = np.array(seq).reshape(1, -1)\n",
        "\n",
        "    preds = model.predict(seq)[0]\n",
        "    top_5 = preds.argsort()[-5:][::-1]\n",
        "\n",
        "    words = []\n",
        "    for i in top_5:\n",
        "        for k, v in tokenizer.word_index.items():\n",
        "            if v == i:\n",
        "                words.append(k)\n",
        "                break\n",
        "\n",
        "    return \", \".join(words)\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=predict_words,\n",
        "    inputs=gr.Textbox(label=\"Enter text\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Next Word Prediction (Gradio)\"\n",
        ")\n",
        "\n",
        "ui.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2FqBFXdxCfI8",
        "outputId": "93f9ed19-d36e-46c6-b982-d397cc0845f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File not found: filepath=nwp_model.keras. Please ensure the file is an accessible `.keras` zip file.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1315820673.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nwp_model.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n\u001b[1;32m    199\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;34mf\"File not found: filepath={filepath}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;34m\"Please ensure the file is an accessible `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File not found: filepath=nwp_model.keras. Please ensure the file is an accessible `.keras` zip file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NL7aVG6hb0fD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}